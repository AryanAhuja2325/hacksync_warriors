from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field, validator
from typing import List, Optional, Dict, Any
import spacy
from textblob import TextBlob
from functools import lru_cache
import logging
import re
from collections import defaultdict
import requests
from io import BytesIO
from pypdf import PdfReader
from langdetect import detect, LangDetectException


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Text Feature Extractor",
    description="Advanced text analysis with context-aware feature extraction",
    version="3.0.0"
)

# ---------------- LAZY LOADING ---------------- #

_nlp = None

def get_nlp():
    """Lazy load spaCy model"""
    global _nlp
    if _nlp is None:
        try:
            _nlp = spacy.load("en_core_web_sm")
            logger.info("Loaded spaCy model successfully")
        except OSError:
            logger.error("spaCy model not found. Run: python -m spacy download en_core_web_sm")
            raise
    return _nlp

# ---------------- ENHANCED CONFIG ---------------- #

# Exclude common platforms from product names
PLATFORM_KEYWORDS = {
    "instagram", "twitter", "linkedin", "youtube", "facebook",
    "tiktok", "web", "website", "mobile app", "android", "ios",
    "email", "whatsapp", "telegram", "slack", "discord", "app store",
    "play store", "chrome", "safari", "firefox"
}

AUDIENCE_PATTERNS = {
    # Direct mentions
    "students": r'\b(students?|learners?|pupils?)\b',
    "developers": r'\b(developers?|programmers?|coders?|engineers?)\b',
    "businesses": r'\b(business(es)?|companies|enterprises?|organizations?)\b',
    "marketers": r'\b(marketers?|marketing (teams?|professionals?))\b',
    "teachers": r'\b(teachers?|educators?|instructors?)\b',
    "customers": r'\b(customers?|clients?|consumers?)\b',
    "freelancers": r'\b(freelancers?|independent contractors?|self-employed)\b',
    "startups": r'\b(startups?|new (businesses|companies))\b',
    "entrepreneurs": r'\b(entrepreneurs?|founders?|business owners?)\b',
    "designers": r'\b(designers?|creatives?|artists?)\b',
    "writers": r'\b(writers?|authors?|content creators?)\b',
    "professionals": r'\b(professionals?|working (people|adults))\b',
    "parents": r'\b(parents?|moms?|dads?|families)\b',
    "children": r'\b(children|kids?|toddlers?)\b',
}

GOAL_PATTERNS = {
    "learn": ["learn", "study", "understand", "master", "discover"],
    "increase": ["increase", "boost", "grow", "raise", "expand"],
    "reduce": ["reduce", "decrease", "minimize", "lower", "cut"],
    "improve": ["improve", "enhance", "better", "upgrade", "refine"],
    "automate": ["automate", "streamline", "simplify", "ease"],
    "track": ["track", "monitor", "measure", "analyze", "follow"],
    "manage": ["manage", "organize", "control", "handle", "coordinate"],
    "create": ["create", "build", "develop", "design", "make"],
    "connect": ["connect", "communicate", "collaborate", "share", "network"],
}

TONE_KEYWORDS = {
    "friendly": ["friendly", "fun", "easy", "simple", "happy", "enjoy", "love"],
    "professional": ["professional", "business", "enterprise", "corporate", "efficient"],
    "casual": ["casual", "relaxed", "chill", "laid-back"],
    "educational": ["learn", "teach", "educational", "tutorial", "course", "study"],
    "motivational": ["achieve", "success", "grow", "transform", "empower"],
}

# ---------------- MODELS ---------------- #

class TextInput(BaseModel):
    text: str = Field(..., min_length=1, max_length=10000)
    
    @validator('text')
    def text_not_empty(cls, v):
        if not v.strip():
            raise ValueError('Text cannot be empty')
        return v.strip()

class ExtractionResponse(BaseModel):
    product: Optional[Dict[str, Any]] = Field(None, description="Detected product with details")
    audience: Dict[str, Any] = Field(default_factory=dict, description="Target audience analysis")
    goals: Dict[str, Any] = Field(default_factory=dict, description="Identified goals with categories")
    tone: Dict[str, Any] = Field(default_factory=dict, description="Tone analysis")
    platform: Dict[str, Any] = Field(default_factory=dict, description="Platform details")
    content_type: Optional[str] = Field(None, description="Type of content (e.g., video, text)")
    key_features: List[str] = Field(default_factory=list, description="Key features mentioned")
    summary: str = Field(..., description="Brief summary of the text")

class SimpleExtractionResponse(BaseModel):
    product: Optional[str]
    audience: Optional[str]
    goals: List[str]
    tone: Optional[str]
    platform: Optional[str]

# ---------------- ADVANCED EXTRACTORS ---------------- #

class AdvancedFeatureExtractor:
    """Enhanced extraction with context awareness"""
    
    @staticmethod
    def extract_product(text: str) -> Optional[Dict[str, Any]]:
        """Extract product with context and filtering"""
        try:
            nlp = get_nlp()
            doc = nlp(text)
            
            candidates = []
            
            # Strategy 1: Look for "we built/created/developed X" patterns
            build_pattern = re.compile(r'\b(built|created|developed|made|designed|launched)\s+(?:a\s+)?([^.,]+?)(?:\s+that|\s+to|\s+for|$)', re.IGNORECASE)
            build_matches = build_pattern.findall(text)
            for _, product in build_matches:
                product = product.strip()
                # Filter out if it's just a platform name
                if product.lower() not in PLATFORM_KEYWORDS and len(product.split()) <= 6:
                    candidates.append(("build_pattern", product, 10))
            
            # Strategy 2: Named entities (but filter platforms)
            for ent in doc.ents:
                if ent.label_ == "PRODUCT":
                    if ent.text.lower() not in PLATFORM_KEYWORDS:
                        candidates.append(("ner_product", ent.text, 8))
            
            # Strategy 3: Noun chunks with descriptors
            for chunk in doc.noun_chunks:
                chunk_text = chunk.text.lower()
                # Look for "a/an/the [adjective] [product_type]"
                if any(word in chunk_text for word in ["app", "platform", "tool", "software", "service", "solution"]):
                    # Make sure it's not just "Android" or "mobile app"
                    if chunk.text.lower() not in PLATFORM_KEYWORDS and len(chunk.text.split()) >= 2:
                        candidates.append(("noun_chunk", chunk.text, 6))
            
            # Strategy 4: Extract from quoted text
            quotes = re.findall(r'["\']([^"\']+)["\']', text)
            for quote in quotes:
                if 3 <= len(quote.split()) <= 6:
                    candidates.append(("quoted", quote, 7))
            
            if not candidates:
                return None
            
            # Sort by score and pick best
            candidates.sort(key=lambda x: x[2], reverse=True)
            best_product = candidates[0][1]
            
            # Extract product category
            category = None
            if "app" in best_product.lower():
                category = "mobile_application"
            elif "platform" in best_product.lower():
                category = "platform"
            elif "tool" in best_product.lower():
                category = "tool"
            elif "software" in best_product.lower():
                category = "software"
            
            return {
                "name": best_product.strip(),
                "category": category,
                "confidence": "high" if candidates[0][2] >= 8 else "medium"
            }
            
        except Exception as e:
            logger.error(f"Error in extract_product: {e}")
            return None
    
    @staticmethod
    def extract_audience(text: str) -> Dict[str, Any]:
        """Advanced audience detection with context"""
        text_lower = text.lower()
        found_audiences = []
        
        # Pattern-based matching
        for audience, pattern in AUDIENCE_PATTERNS.items():
            if re.search(pattern, text_lower):
                found_audiences.append(audience)
        
        # Context analysis - look for helping/targeting patterns
        help_pattern = re.compile(r'\b(help|helps|for|targeting|designed for)\s+([^.,]+)', re.IGNORECASE)
        help_matches = help_pattern.findall(text)
        
        primary_audience = None
        if found_audiences:
            # The first mentioned is usually primary
            primary_audience = found_audiences[0]
        
        # Extract demographic info if present
        demographics = {
            "age_group": None,
            "skill_level": None,
            "context": []
        }
        
        # Age detection
        if re.search(r'\b(kids?|children|young)\b', text_lower):
            demographics["age_group"] = "children"
        elif re.search(r'\b(students?|learners?)\b', text_lower):
            demographics["age_group"] = "young_adults"
        elif re.search(r'\b(professionals?|working)\b', text_lower):
            demographics["age_group"] = "adults"
        
        # Skill level
        if re.search(r'\b(beginner|new|novice|starting)\b', text_lower):
            demographics["skill_level"] = "beginner"
        elif re.search(r'\b(intermediate|learning)\b', text_lower):
            demographics["skill_level"] = "intermediate"
        elif re.search(r'\b(advanced|expert|professional)\b', text_lower):
            demographics["skill_level"] = "advanced"
        
        return {
            "primary": primary_audience,
            "all_mentioned": found_audiences,
            "demographics": demographics,
            "count": len(found_audiences)
        }
    
    @staticmethod
    def extract_goals(text: str) -> Dict[str, Any]:
        """Extract and categorize goals"""
        nlp = get_nlp()
        doc = nlp(text)
        
        categorized_goals = defaultdict(list)
        all_goals = []
        
        # Analyze each sentence
        for sent in doc.sents:
            sent_text = sent.text.strip()
            sent_lower = sent_text.lower()
            
            # Check which goal category this matches
            for category, verbs in GOAL_PATTERNS.items():
                if any(verb in sent_lower for verb in verbs):
                    # Extract the actual goal (remove redundant parts)
                    goal_clean = sent_text
                    categorized_goals[category].append(goal_clean)
                    all_goals.append({
                        "text": goal_clean,
                        "category": category
                    })
                    break
        
        # Extract primary goal (usually the main verb phrase)
        primary_goal = None
        if all_goals:
            primary_goal = all_goals[0]["text"]
        
        return {
            "primary": primary_goal,
            "all": all_goals[:5],  # Limit to 5
            "categorized": dict(categorized_goals),
            "count": len(all_goals)
        }
    
    @staticmethod
    def extract_tone(text: str) -> Dict[str, Any]:
        """Advanced tone analysis"""
        try:
            blob = TextBlob(text)
            polarity = blob.sentiment.polarity
            subjectivity = blob.sentiment.subjectivity
            
            text_lower = text.lower()
            
            # Keyword-based tone detection
            tone_scores = {}
            for tone, keywords in TONE_KEYWORDS.items():
                score = sum(1 for kw in keywords if kw in text_lower)
                if score > 0:
                    tone_scores[tone] = score
            
            # Determine primary tone
            primary_tone = "neutral"
            if tone_scores:
                primary_tone = max(tone_scores, key=tone_scores.get)
            elif polarity > 0.3:
                primary_tone = "enthusiastic"
            elif polarity < -0.3:
                primary_tone = "serious"
            elif subjectivity > 0.6:
                primary_tone = "casual"
            else:
                primary_tone = "professional"
            
            # Additional tone characteristics
            characteristics = []
            if "!" in text:
                characteristics.append("enthusiastic")
            if "?" in text:
                characteristics.append("inquisitive")
            if re.search(r'\b(help|support|assist)\b', text_lower):
                characteristics.append("helpful")
            
            return {
                "primary": primary_tone,
                "sentiment": {
                    "polarity": round(polarity, 3),
                    "subjectivity": round(subjectivity, 3),
                    "label": "positive" if polarity > 0.1 else "negative" if polarity < -0.1 else "neutral"
                },
                "characteristics": list(set(characteristics)),
                "formality": "informal" if subjectivity > 0.5 else "formal"
            }
        except Exception as e:
            logger.error(f"Error in extract_tone: {e}")
            return {"primary": "neutral", "sentiment": {"polarity": 0, "subjectivity": 0}}
    
    @staticmethod
    def extract_platform(text: str) -> Dict[str, Any]:
        """Detailed platform extraction"""
        text_lower = text.lower()
        found_platforms = []
        
        for platform in PLATFORM_KEYWORDS:
            if platform in text_lower:
                found_platforms.append(platform)
        
        # Categorize platforms
        categories = {
            "mobile": ["android", "ios", "mobile app"],
            "social": ["instagram", "twitter", "linkedin", "facebook", "tiktok"],
            "web": ["web", "website", "chrome", "safari", "firefox"],
            "messaging": ["whatsapp", "telegram", "slack", "discord", "email"]
        }
        
        platform_categories = []
        for category, platforms in categories.items():
            if any(p in found_platforms for p in platforms):
                platform_categories.append(category)
        
        # Determine primary platform
        primary = found_platforms[0] if found_platforms else None
        
        return {
            "primary": primary,
            "all_mentioned": found_platforms,
            "categories": platform_categories,
            "count": len(found_platforms)
        }
    
    @staticmethod
    def extract_content_type(text: str) -> Optional[str]:
        """Detect type of content mentioned"""
        text_lower = text.lower()
        
        content_types = {
            "video": r'\b(video|videos|clips?|recordings?)\b',
            "text": r'\b(articles?|blogs?|posts?|text)\b',
            "audio": r'\b(audio|podcast|music|sound)\b',
            "images": r'\b(images?|photos?|pictures?|graphics?)\b',
            "interactive": r'\b(interactive|games?|quizzes?)\b',
        }
        
        for content_type, pattern in content_types.items():
            if re.search(pattern, text_lower):
                return content_type
        
        return None
    
    @staticmethod
    def extract_key_features(text: str) -> List[str]:
        """Extract key features or benefits"""
        features = []
        text_lower = text.lower()
        
        # Look for feature indicators
        feature_patterns = [
            r'\b(using|with|features?|includes?|provides?|offers?)\s+([^.,]+)',
            r'\b(short|quick|easy|simple|fast|secure|free)\s+([a-z]+)',
        ]
        
        for pattern in feature_patterns:
            matches = re.findall(pattern, text_lower)
            for match in matches:
                if isinstance(match, tuple):
                    feature = ' '.join(match).strip()
                else:
                    feature = match.strip()
                if feature and len(feature.split()) <= 4:
                    features.append(feature)
        
        # Remove duplicates and limit
        return list(set(features))[:5]
    
    @staticmethod
    def generate_summary(text: str, max_length: int = 100) -> str:
        """Generate a concise summary"""
        nlp = get_nlp()
        doc = nlp(text)
        
        # Extract key sentence (usually first or one with most entities)
        sentences = list(doc.sents)
        if not sentences:
            return text[:max_length]
        
        # Score sentences by entity count and position
        scored_sentences = []
        for i, sent in enumerate(sentences):
            score = len(sent.ents) * 2  # Entities are important
            if i == 0:
                score += 3  # First sentence bonus
            scored_sentences.append((score, sent.text))
        
        scored_sentences.sort(reverse=True, key=lambda x: x[0])
        summary = scored_sentences[0][1].strip()
        
        if len(summary) > max_length:
            summary = summary[:max_length] + "..."
        
        return summary

# ---------------- API ENDPOINTS ---------------- #

@app.get("/")
async def root():
    return {
        "status": "healthy",
        "service": "Advanced Text Feature Extractor API",
        "version": "3.0.0"
    }

@app.post("/extract", response_model=ExtractionResponse)
async def extract_features(data: TextInput):
    """
    Advanced feature extraction with context analysis
    """
    try:
        text = data.text
        extractor = AdvancedFeatureExtractor()
        
        return ExtractionResponse(
            product=extractor.extract_product(text),
            audience=extractor.extract_audience(text),
            goals=extractor.extract_goals(text),
            tone=extractor.extract_tone(text),
            platform=extractor.extract_platform(text),
            content_type=extractor.extract_content_type(text),
            key_features=extractor.extract_key_features(text),
            summary=extractor.generate_summary(text)
        )
    
    except Exception as e:
        logger.error(f"Extraction error: {e}")
        raise HTTPException(status_code=500, detail=f"Extraction failed: {str(e)}")

@app.post("/extract/batch")
async def extract_features_batch(texts: List[TextInput]):
    """Batch processing endpoint"""
    if len(texts) > 100:
        raise HTTPException(status_code=400, detail="Maximum 100 texts per batch")
    
    results = []
    extractor = AdvancedFeatureExtractor()
    
    for item in texts:
        try:
            results.append({
                "input": item.text[:100] + "..." if len(item.text) > 100 else item.text,
                "features": ExtractionResponse(
                    product=extractor.extract_product(item.text),
                    audience=extractor.extract_audience(item.text),
                    goals=extractor.extract_goals(item.text),
                    tone=extractor.extract_tone(item.text),
                    platform=extractor.extract_platform(item.text),
                    content_type=extractor.extract_content_type(item.text),
                    key_features=extractor.extract_key_features(item.text),
                    summary=extractor.generate_summary(item.text)
                )
            })
        except Exception as e:
            logger.error(f"Error processing text: {e}")
            results.append({"input": item.text[:100], "error": str(e)})
    
    return {"results": results, "total": len(results)}


def filter_output(adv: ExtractionResponse) -> SimpleExtractionResponse:
    goal = adv.goals.get("primary")

    clean_goal = None
    if goal:
        g = goal.lower()
        g = re.sub(r"we built|this app|helps?", "", g)
        g = re.sub(r"\b(using|with|via)\b.*", "", g)
        clean_goal = g.strip()

    return SimpleExtractionResponse(
        product=adv.product.get("name") if adv.product else None,
        audience=adv.audience.get("primary"),
        goals=[clean_goal] if clean_goal else [],
        tone=adv.tone.get("primary"),
        platform=adv.platform.get("primary")
    )

@app.post("/extract/simple", response_model=SimpleExtractionResponse)
async def extract_simple(data: TextInput):
    advanced = await extract_features(data)
    return filter_output(advanced)

@app.on_event("startup")
async def startup_event():
    logger.info("Starting Advanced Text Feature Extractor API...")
    try:
        get_nlp()
        logger.info("API ready")
    except Exception as e:
        logger.error(f"Failed to load models: {e}")


def pdf_url_to_text(pdf_url: str) -> str:
    try:
        response = requests.get(pdf_url, timeout=15)
        if response.status_code != 200:
            raise HTTPException(status_code=400, detail="Failed to download PDF")

        reader = PdfReader(BytesIO(response.content))
        text_chunks = []

        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text_chunks.append(page_text)

        final_text = "\n".join(text_chunks).strip()
        if not final_text:
            raise HTTPException(status_code=400, detail="No extractable text found in PDF")

        return final_text

    except requests.exceptions.RequestException:
        raise HTTPException(status_code=400, detail="Invalid or unreachable PDF URL")
    except Exception as e:
        logger.error(f"PDF parsing error: {e}")
        raise HTTPException(status_code=400, detail="Failed to parse PDF")

class PdfUrlInput(BaseModel):
    pdf_url: str = Field(..., min_length=10)


@app.post("/extract/pdf-url", response_model=SimpleExtractionResponse)
async def extract_from_pdf_url(data: PdfUrlInput):
    """
    Extract features from a PDF URL.
    PDF → text → same NLP pipeline
    """
    text = pdf_url_to_text(data.pdf_url)

    advanced = await extract_features(TextInput(text=text))
    return filter_output(advanced)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)